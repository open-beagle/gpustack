From 555f683fdb0ecef52ba1210acd67c6afb58c1791 Mon Sep 17 00:00:00 2001
From: shucheng <shucheng@bd-apaas.com>
Date: Sat, 11 Jan 2025 10:30:10 +0800
Subject: [PATCH] v0.4.2-s3

---
 gpustack/cmd/start.py            |  86 ++++++++++-----
 gpustack/config/config.py        |   4 +
 gpustack/scheduler/calculator.py |  35 +++++-
 gpustack/worker/backends/base.py |  29 ++++-
 gpustack/worker/downloader_s3.py | 177 +++++++++++++++++++++++++++++++
 5 files changed, 298 insertions(+), 33 deletions(-)
 create mode 100644 gpustack/worker/downloader_s3.py

diff --git a/gpustack/cmd/start.py b/gpustack/cmd/start.py
index d594d60..c401fbe 100644
--- a/gpustack/cmd/start.py
+++ b/gpustack/cmd/start.py
@@ -42,51 +42,51 @@ def setup_start_cmd(subparsers: argparse._SubParsersAction):
         "--config-file",
         type=str,
         help="Path to the YAML config file.",
-        default=get_gpustack_env("CONFIG_FILE"),
+        default=get_gpustack_env("STACK_CONFIG_FILE"),
     )
     group.add_argument(
         "-d",
         "--debug",
         action=OptionalBoolAction,
         help="Enable debug mode.",
-        default=get_gpustack_env_bool("DEBUG"),
+        default=get_gpustack_env_bool("STACK_DEBUG"),
     )
     group.add_argument(
         "--data-dir",
         type=str,
         help="Directory to store data. Default is OS specific.",
-        default=get_gpustack_env("DATA_DIR"),
+        default=get_gpustack_env("STACK_DATA_DIR"),
     )
     group.add_argument(
         "--cache-dir",
         type=str,
         help="Directory to store cache (e.g., model files). Defaults to <data-dir>/cache.",
-        default=get_gpustack_env("CACHE_DIR"),
+        default=get_gpustack_env("STACK_CACHE_DIR"),
     )
     group.add_argument(
         "--bin-dir",
         type=str,
         help="Directory to store additional binaries, e.g., versioned backend executables.",
-        default=get_gpustack_env("BIN_DIR"),
+        default=get_gpustack_env("STACK_BIN_DIR"),
     )
     group.add_argument(
         "--pipx-path",
         type=str,
         help="Path to the pipx executable, used to install versioned backends.",
-        default=get_gpustack_env("PIPX_PATH"),
+        default=get_gpustack_env("STACK_PIPX_PATH"),
     )
     group.add_argument(
         "-t",
         "--token",
         type=str,
         help="Shared secret used to add a worker.",
-        default=get_gpustack_env("TOKEN"),
+        default=get_gpustack_env("STACK_TOKEN"),
     )
     group.add_argument(
         "--huggingface-token",
         type=str,
         help="User Access Token to authenticate to the Hugging Face Hub.",
-        default=os.getenv("HF_TOKEN"),
+        default=os.getenv("STACK_HF_TOKEN"),
     )
 
     group = parser_server.add_argument_group("Server settings")
@@ -94,68 +94,68 @@ def setup_start_cmd(subparsers: argparse._SubParsersAction):
         "--host",
         type=str,
         help="Host to bind the server to.",
-        default=get_gpustack_env("HOST"),
+        default=get_gpustack_env("STACK_HOST"),
     )
     group.add_argument(
         "--port",
         type=int,
         help="Port to bind the server to.",
-        default=get_gpustack_env("PORT"),
+        default=get_gpustack_env("STACK_PORT"),
     )
     group.add_argument(
         "--database-url",
         type=str,
         help="URL of the database. Example: postgresql://user:password@hostname:port/db_name.",
-        default=get_gpustack_env("DATABASE_URL"),
+        default=get_gpustack_env("STACK_DATABASE_URL"),
     )
     group.add_argument(
         "--disable-worker",
         action=OptionalBoolAction,
         help="Disable embedded worker.",
-        default=get_gpustack_env_bool("DISABLE_WORKER"),
+        default=get_gpustack_env_bool("STACK_DISABLE_WORKER"),
     )
     group.add_argument(
         "--bootstrap-password",
         type=str,
         help="Initial password for the default admin user. Random by default.",
-        default=get_gpustack_env("BOOTSTRAP_PASSWORD"),
+        default=get_gpustack_env("STACK_BOOTSTRAP_PASSWORD"),
     )
     group.add_argument(
         "--ssl-keyfile",
         type=str,
         help="Path to the SSL key file.",
-        default=get_gpustack_env("SSL_KEYFILE"),
+        default=get_gpustack_env("STACK_SSL_KEYFILE"),
     )
     group.add_argument(
         "--ssl-certfile",
         type=str,
         help="Path to the SSL certificate file.",
-        default=get_gpustack_env("SSL_CERTFILE"),
+        default=get_gpustack_env("STACK_SSL_CERTFILE"),
     )
     group.add_argument(
         "--force-auth-localhost",
         action=OptionalBoolAction,
         help="Force authentication for requests originating from localhost (127.0.0.1)."
         "When set to True, all requests from localhost will require authentication.",
-        default=get_gpustack_env_bool("FORCE_AUTH_LOCALHOST"),
+        default=get_gpustack_env_bool("STACK_FORCE_AUTH_LOCALHOST"),
     )
     group.add_argument(
         "--ollama-library-base-url",
         type=str,
         help="Base URL of the Ollama library. Default is https://registry.ollama.ai.",
-        default=get_gpustack_env("OLLAMA_LIBRARY_BASE_URL"),
+        default=get_gpustack_env("STACK_OLLAMA_LIBRARY_BASE_URL"),
     )
     group.add_argument(
         "--disable-update-check",
         action=OptionalBoolAction,
         help="Disable update check.",
-        default=get_gpustack_env_bool("DISABLE_UPDATE_CHECK"),
+        default=get_gpustack_env_bool("STACK_DISABLE_UPDATE_CHECK"),
     )
     group.add_argument(
         "--update-check-url",
         type=str,
         help=argparse.SUPPRESS,
-        default=get_gpustack_env("UPDATE_CHECK_URL"),
+        default=get_gpustack_env("STACK_UPDATE_CHECK_URL"),
     )
 
     group = parser_server.add_argument_group("Worker settings")
@@ -164,26 +164,26 @@ def setup_start_cmd(subparsers: argparse._SubParsersAction):
         "--server-url",
         type=str,
         help="Server to connect to.",
-        default=get_gpustack_env("SERVER_URL"),
+        default=get_gpustack_env("STACK_SERVER_URL"),
     )
     group.add_argument(
         "--worker-ip",
         type=str,
         help="IP address of the worker node. Auto-detected by default.",
-        default=get_gpustack_env("WORKER_IP"),
+        default=get_gpustack_env("STACK_WORKER_IP"),
     )
     group.add_argument(
         "--worker-name",
         type=str,
         help="Name of the worker node. Use the hostname by default.",
-        default=get_gpustack_env("WORKER_NAME"),
+        default=get_gpustack_env("STACK_WORKER_NAME"),
     )
     group.add_argument(
         "--disable-metrics",
         action=OptionalBoolAction,
         help="Disable metrics.",
         default=get_gpustack_env_bool(
-            "DISABLE_METRICS",
+            "STACK_DISABLE_METRICS",
         ),
     )
     group.add_argument(
@@ -191,26 +191,26 @@ def setup_start_cmd(subparsers: argparse._SubParsersAction):
         action=OptionalBoolAction,
         help="Disable RPC servers.",
         default=get_gpustack_env_bool(
-            "DISABLE_METRICS",
+            "STACK_DISABLE_METRICS",
         ),
     )
     group.add_argument(
         "--metrics-port",
         type=int,
         help="Port to expose metrics.",
-        default=get_gpustack_env("METRICS_PORT"),
+        default=get_gpustack_env("STACK_METRICS_PORT"),
     )
     group.add_argument(
         "--worker-port",
         type=int,
         help="Port to bind the worker to.",
-        default=get_gpustack_env("WORKER_PORT"),
+        default=get_gpustack_env("STACK_WORKER_PORT"),
     )
     group.add_argument(
         "--log-dir",
         type=str,
         help="Directory to store logs.",
-        default=get_gpustack_env("LOG_DIR"),
+        default=get_gpustack_env("STACK_LOG_DIR"),
     )
     group.add_argument(
         "--system-reserved",
@@ -220,13 +220,37 @@ def setup_start_cmd(subparsers: argparse._SubParsersAction):
         By default, 2 GiB of RAM is reserved. \
         Example: '{\"ram\": 2, \"vram\": 0}' or '{\"memory\": 2, \"gpu_memory\": 0}', \
         Note: The 'memory' and 'gpu_memory' keys are deprecated and will be removed in future releases.",
-        default=get_gpustack_env("SYSTEM_RESERVED"),
+        default=get_gpustack_env("STACK_SYSTEM_RESERVED"),
     )
     group.add_argument(
         "--tools-download-base-url",
         type=str,
         help="Base URL to download dependency tools.",
-        default=get_gpustack_env("TOOLS_DOWNLOAD_BASE_URL"),
+        default=get_gpustack_env("STACK_TOOLS_DOWNLOAD_BASE_URL"),
+    )
+    group.add_argument(
+        "--worker-s3-host",
+        type=str,
+        help="HOST to s3.",
+        default=get_gpustack_env("STACK_WORKER_S3_HOST"),
+    )
+    group.add_argument(
+        "--worker-s3-access-key",
+        type=str,
+        help="AccessKey to s3.",
+        default=get_gpustack_env("STACK_WORKER_S3_ACCESS_KEY"),
+    )
+    group.add_argument(
+        "--worker-s3-secret-key",
+        type=str,
+        help="SecretKey to s3.",
+        default=get_gpustack_env("STACK_WORKER_S3_SECRET_KEY"),
+    )
+    group.add_argument(
+        "--worker-s3-ssl",
+        action=OptionalBoolAction,
+        help="SecretKey to s3.",
+        default=get_gpustack_env_bool("STACK_WORKER_S3_SSL") or False,
     )
 
     parser_server.set_defaults(func=run)
@@ -314,6 +338,10 @@ def set_common_options(args, config_data: dict):
         "pipx_path",
         "token",
         "huggingface_token",
+        "worker_s3_host",
+        "worker_s3_access_key",
+        "worker_s3_secret_key",
+        "worker_s3_ssl",
     ]
 
     for option in options:
diff --git a/gpustack/config/config.py b/gpustack/config/config.py
index 6b49d8b..8c852cf 100644
--- a/gpustack/config/config.py
+++ b/gpustack/config/config.py
@@ -87,6 +87,10 @@ class Config(BaseSettings):
     resources: Optional[dict] = None
     bin_dir: Optional[str] = None
     pipx_path: Optional[str] = None
+    worker_s3_host: Optional[str] = ""
+    worker_s3_access_key: Optional[str] = ""
+    worker_s3_secret_key: Optional[str] = ""
+    worker_s3_ssl: bool = False
 
     def __init__(self, **values):
         super().__init__(**values)
diff --git a/gpustack/scheduler/calculator.py b/gpustack/scheduler/calculator.py
index fdaf469..7ba0f39 100644
--- a/gpustack/scheduler/calculator.py
+++ b/gpustack/scheduler/calculator.py
@@ -241,7 +241,25 @@ async def calculate_model_resource_claim(
         model: Model to calculate the resource claim for.
     """
 
-    if model.source == SourceEnum.LOCAL_PATH and not os.path.exists(model.local_path):
+    # 获取cache_dir的父目录
+    cache_dir = kwargs.get("cache_dir")
+    if cache_dir:
+        cache_dir = os.path.dirname(cache_dir)  # 获取上一级目录
+    cache_path = model.local_path
+    if model.source == SourceEnum.LOCAL_PATH and model.local_path.startswith(
+        "s3://beagle_wind/"
+    ):
+        # 构建本地缓存路径
+        if cache_dir:
+            cache_path = os.path.join(
+                cache_dir,
+                "beagle",
+                model.local_path.removeprefix(
+                    "s3://beagle_wind/bd-wind/datamodel/"
+                ),
+            )
+
+    if model.source == SourceEnum.LOCAL_PATH and not os.path.exists(cache_path):
         # Skip the calculation if the model is not available, policies like spread strategy still apply.
         # TODO Support user provided resource claim for better scheduling.
         estimate = _get_empty_estimate()
@@ -355,7 +373,20 @@ async def _gguf_parser_command_args_from_source(  # noqa: C901
             )
             return ["-ms-repo", model.model_scope_model_id, "-ms-file", file_path]
         elif model.source == SourceEnum.LOCAL_PATH:
-            return ["--path", model.local_path]
+            # 获取cache_dir的父目录
+            cache_dir = kwargs.get("cache_dir")
+            cache_path = model.local_path
+            if cache_dir:
+                cache_dir = os.path.dirname(cache_dir)  # 获取上一级目录
+            if cache_dir and model.local_path.startswith("s3://beagle_wind/"):
+                cache_path = os.path.join(
+                    cache_dir,
+                    "beagle",
+                    model.local_path.removeprefix(
+                        "s3://beagle_wind/bd-wind/datamodel/"
+                    ),
+                )
+            return ["--path", cache_path]
     except asyncio.TimeoutError:
         raise Exception(f"Timeout when getting the file for model {model.name}")
     except Exception as e:
diff --git a/gpustack/worker/backends/base.py b/gpustack/worker/backends/base.py
index e5e8d19..bb7bf85 100644
--- a/gpustack/worker/backends/base.py
+++ b/gpustack/worker/backends/base.py
@@ -6,6 +6,7 @@ import threading
 import time
 from typing import List, Optional
 from abc import ABC, abstractmethod
+from pathlib import Path
 
 from gpustack.client.generated_clientset import ClientSet
 from gpustack.config.config import Config
@@ -25,9 +26,14 @@ from gpustack.worker.downloaders import (
     OllamaLibraryDownloader,
 )
 from gpustack.worker.tools_manager import ToolsManager
+from gpustack.worker.downloader_s3 import S3Downloader
+
+from minio import Minio
+from minio.error import S3Error
 
 logger = logging.getLogger(__name__)
 lock = threading.Lock()
+s3Downloader = None
 
 ACCELERATOR_VENDOR_TO_ENV_NAME = {
     VendorEnum.NVIDIA: "CUDA_VISIBLE_DEVICES",
@@ -94,7 +100,11 @@ def download_model(
             cache_dir=os.path.join(cache_dir, "model_scope"),
         )
     elif mi.source == SourceEnum.LOCAL_PATH:
-        return mi.local_path
+        if 's3://beagle_wind' in mi.local_path:
+            # example s3://beagle_wind/bd-wind/datamodel/51c63609-faf4-446a-a2cf-47dd8d1a3e97/v1
+            return s3Downloader.download(mi.local_path)
+        else:
+            return mi.local_path
 
 
 def get_model_file_size(mi: ModelInstance, cfg: Config) -> Optional[int]:
@@ -107,6 +117,10 @@ def get_model_file_size(mi: ModelInstance, cfg: Config) -> Optional[int]:
         return ModelScopeDownloader.get_model_file_size(
             model_instance=mi,
         )
+    elif mi.source == SourceEnum.LOCAL_PATH and 's3://beagle_wind' in mi.local_path:
+        return s3Downloader.get_model_file_size(
+            model_instance=mi,
+        )
 
     return None
 
@@ -141,6 +155,8 @@ class InferenceServer(ABC):
     ):
         setup_logging(debug=cfg.debug)
 
+        self.init_s3_client(cfg)
+
         model_file_size = get_model_file_size(mi, cfg)
         if model_file_size:
             logger.debug(f"Model file size: {model_file_size}")
@@ -149,7 +165,6 @@ class InferenceServer(ABC):
         # for download progress update frequency control
         self._last_download_update_time = time.time()
         self.hijack_tqdm_progress()
-
         self._clientset = clientset
         self._model_instance = mi
         self._config = cfg
@@ -322,6 +337,16 @@ class InferenceServer(ABC):
             # TODO: support more.
             return None
 
+    def init_s3_client(self, cfg):
+        global s3Downloader
+        s3Downloader = S3Downloader(
+            cfg.worker_s3_host,
+            access_key=cfg.worker_s3_access_key,
+            secret_key=cfg.worker_s3_secret_key,
+            ssl=cfg.worker_s3_ssl,
+            cache_dir=os.path.join(cfg.cache_dir, "beagle"),
+        )
+
 
 def get_env_name_by_vendor(vendor: str) -> str:
     env_name = next(
diff --git a/gpustack/worker/downloader_s3.py b/gpustack/worker/downloader_s3.py
new file mode 100644
index 0000000..ecf6901
--- /dev/null
+++ b/gpustack/worker/downloader_s3.py
@@ -0,0 +1,177 @@
+import os
+import logging
+from pathlib import Path
+from typing import Optional
+from minio import Minio
+from minio.error import S3Error
+from filelock import FileLock
+from tqdm import tqdm
+
+from gpustack.schemas.models import (
+    ModelInstance,
+)
+
+logger = logging.getLogger(__name__)
+
+
+class S3Downloader:
+    _default_cache_dir = "/var/lib/gpustack/cache/beagle"
+
+    def __init__(
+        self,
+        host: str,
+        access_key: str,
+        secret_key: str,
+        ssl: bool = True,
+        cache_dir: Optional[str] = None,
+    ):
+        self._s3_client = Minio(
+            host, access_key=access_key, secret_key=secret_key, secure=ssl
+        )
+        self._cache_dir = cache_dir or self._default_cache_dir
+
+    def download(
+        self,
+        s3_path: str,
+        cache_dir: Optional[str] = None,
+    ) -> str:
+        """从S3下载模型文件
+
+        Args:
+            s3_path: S3路径,格式为s3://bucket/path/to/model
+            cache_dir: 本地缓存目录
+
+        Returns:
+            下载后的本地文件路径
+        """
+        if not s3_path.startswith("s3://beagle_wind/"):
+            return s3_path
+
+        # 解析S3路径
+        # s3://beagle_wind/bd-wind/datamodel/4c3c6c88-912c-48da-910c-fea84da1fedc/v1/qwen2.5-3b-instruct-q8_0.gguf
+        base_path = s3_path.removeprefix("s3://beagle_wind/")
+        bucket_name = base_path.split("/")[0]
+        object_prefix = "/".join(base_path.split("/")[1:-1])  # 去掉最后的文件名部分
+        filename = base_path.split("/")[-1]  # 获取文件名
+
+        # 准备本地缓存路径
+        local_cache = cache_dir or self._cache_dir
+        local_path = os.path.join(
+            local_cache, s3_path.removeprefix("s3://beagle_wind/bd-wind/datamodel/")
+        )
+        local_dir = os.path.dirname(local_path)  # 获取local_path的父目录
+        if not os.path.exists(local_dir):  # 创建父目录
+            os.makedirs(local_dir)
+
+        # 使用文件锁避免并发下载
+        lock_filename = local_dir + ".lock"
+        logger.debug("获取文件锁")
+
+        with FileLock(lock_filename):
+            try:
+                # 列出所有对象
+                objects = self._s3_client.list_objects(
+                    bucket_name, prefix=object_prefix, recursive=True
+                )
+
+                # 下载每个对象
+                for obj in objects:
+                    self._download_object(
+                        bucket_name=bucket_name,
+                        object_name=obj.object_name,
+                        local_path=os.path.join(
+                            local_cache, obj.object_name.removeprefix("datamodel/")
+                        ),
+                        total_size=obj.size,
+                    )
+
+                logger.debug(f"已下载 {bucket_name}/{object_prefix}")
+                return local_path
+
+            except S3Error as e:
+                logger.error(f"S3下载错误: {e}")
+                raise
+
+    def _download_object(
+        self, bucket_name: str, object_name: str, local_path: str, total_size: int
+    ):
+        """分片下载单个S3对象"""
+        downloaded_size = 0
+        if os.path.exists(local_path):
+            downloaded_size = os.path.getsize(local_path)
+
+        if downloaded_size >= total_size:
+            logger.debug(f"文件 {object_name} 已存在,跳过下载")
+            return
+
+        Path(local_path).parent.mkdir(parents=True, exist_ok=True)
+
+        # 如果是新下载，先清空文件
+        if downloaded_size == 0:
+            open(local_path, 'wb').close()
+
+        part_size = 10 * 1024 * 1024  # 10MB
+        start_byte = downloaded_size
+
+        with tqdm(
+            total=total_size,
+            initial=downloaded_size,
+            desc=object_name,
+            unit='B',
+            unit_scale=True,
+        ) as pbar:
+            while start_byte < total_size:
+                response = self._s3_client.get_object(
+                    bucket_name,
+                    object_name,
+                    offset=start_byte,
+                    length=min(part_size, total_size - start_byte),
+                )
+
+                data = response.read()
+
+                # 始终使用追加模式
+                with open(local_path, "ab") as f:
+                    f.write(data)
+
+                chunk_size = len(data)
+                start_byte += chunk_size
+                pbar.update(chunk_size)
+
+    def get_model_file_size(self, model_instance: ModelInstance) -> int:
+        """获取S3上模型文件的总大小
+
+        Args:
+            model_instance: 模型实例对象
+
+        Returns:
+            文件总大小(字节)
+
+        Raises:
+            S3Error: 当S3操作失败时抛出
+        """
+        if not model_instance.local_path.startswith("s3://beagle_wind/"):
+            return None
+
+        try:
+            # 解析S3路径
+            base_path = model_instance.local_path.removeprefix("s3://beagle_wind/")
+            bucket_name = base_path.split("/")[0]
+            object_prefix = "/".join(base_path.split("/")[1:-1])  # 去掉最后的文件名部分
+
+            # 列出所有对象
+            objects = self._s3_client.list_objects(
+                bucket_name, prefix=object_prefix, recursive=True
+            )
+
+            # 计算所有对象的总大小
+            total_size = sum(obj.size for obj in objects)
+
+            logger.debug(
+                f"S3路径 {model_instance.local_path} 的总大小: {total_size} 字节"
+            )
+            return total_size
+
+        except S3Error as e:
+            logger.error(f"获取S3文件大小失败: {e}")
+            raise
-- 
2.39.2

